<!doctype html>
<html  lang="en" >
<head>
    <style>
		#TOC {
			overflow-y: hidden !important; 
			font-size: smaller !important;
			/*margin-right: 20px;*/
		}

		p, h1, h2, h3, h4, h5, a, span, li, ul, ol, th, tr, table, figcaption {
			font-family: system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Open Sans","Helvetica Neue",sans-serif !important;
		}

		[class*="span"] {
			margin-left: 0 !important;
		}

		.well {
			min-height: 0px !important;
			padding: 0px !important;
			margin-bottom: 0px !important;
			background-color: #FFFFFF  !important;
			border: 0 !important;
			-webkit-border-radius: 0 !important;
			-moz-border-radius: 0 !important;
			border-radius: 0 !important;
			-webkit-box-shadow: none !important;
			-moz-box-shadow: none !important;
			box-shadow: none !important;
		}

		.navbar-inner {
			background-color: #fefefe !important;
			background-image: none !important;
			background-repeat: no-repeat !important;
			filter: none !important;
			border: 0 !important;
			-webkit-border-radius: 0px !important;
			-moz-border-radius: 0px !important;
			border-radius: 0px !important;
			margin-bottom: 15pt !important;

			-webkit-box-shadow: 0 1px 10px rgb(0 0 0 / 7%) !important;
			-moz-box-shadow: 0 1px 10px rgba(0,0,0,.07) 1 !important;
			box-shadow: 0 1px 10px rgb(0 0 0 / 7%) !important;
		}
		
		
		li > ul {
			padding-left: 15px !important;
		}

		pre {
			background-color: #f6f8fa !important;
			border-radius: 3px !important;
			/*font-size: 85% !important;*/
			line-height: 1.45 !important;
			overflow: auto !important;
			padding: 16px !important;
			font-family: Monaco, Menlo, Consolas, "Courier New", monospace !important;
			border: 0 !important;
		}
		
		code {
			/*background-color: rgba(27,31,35,.05) !important;*/
			border-radius: 3px !important;
			border: 0 !important;
			/*font-size: 85% !important;*/
			margin: 0 !important;
			padding: 0.2em 0.4em !important;
			font-family: Monaco, Menlo, Consolas, "Courier New", monospace !important;
		}

		table th {
			background-color: #f6f8fa !important;
		}

		
		.math * {
			font-family: Georgia, Palatino, 'Palatino Linotype', Times, 'Times New Roman', serif !important;
		}

		.katex-display>.katex>.katex-html {
			font-size: 85% !important;
			font-family: Georgia, Palatino, 'Palatino Linotype', Times, 'Times New Roman', serif !important;
		}

		/*distanzia*/
		h1:not(:first-of-type){
			margin-top: 50px !important;
		}

		/* fa comparire la barra del capitolo */
		h1 {
			border-bottom: 1px solid #e8e8e8fa !important;
			font-size: 2.2em !important;
		}

		/*distanzia*/
		h2 {
			margin-top: 30px !important;
			font-size: 1.6em !important;
		}

		/*distanzia*/
		h3 {
			margin-top: 20px !important;
			font-size: 1.3em !important;
		}

		h4 {
			font-size: 1em !important;
		}

		/* stile box */
		.note, .tip, .caution, .warning, .attention, .error, .danger, .definition {
			border-radius: 5pt;
			padding-left: 10px;
			padding-right: 10px;
			padding-top: 1px;
			padding-bottom: 1px;
			line-height: normal;
			color: #000000ab;
			margin: 6pt 0pt 6pt 0pt;
		}

		.note code, .tip code, .caution code, .warning code, .attention code, .error code, .danger code, .definition code, .danger code {
			background-color: #00000014;
		}

		.note {
			background-color: #88d3f9;
		}

		.tip {
			background-color: #87f0b8;
		}

		.caution, .warning, .attention {
			background-color: #ffe162;
		}

		.error, .danger {
			background-color: #ff7474;
		}

		.definition {
			background-color: #f191ff;
		}

		.table {
			border-top: 1px solid #ddd;
			margin-top: 15px;
		}


	</style>

    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <!--[if lt IE 9]>
                <script src="http://css3-mediaqueries-js.googlecode.com/svn/trunk/css3-mediaqueries.js"></script>
        <![endif]-->
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta http-equiv="Content-Style-Type" content="text/css" />

    <!-- <link rel="stylesheet" type="text/css" href="template.css" /> -->
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/template.css" />

    <link href="https://vjs.zencdn.net/5.4.4/video-js.css" rel="stylesheet" />

    <script src="https://code.jquery.com/jquery-2.2.1.min.js"></script>
    <!-- <script type='text/javascript' src='menu/js/jquery.cookie.js'></script> -->
    <!-- <script type='text/javascript' src='menu/js/jquery.hoverIntent.minified.js'></script> -->
    <!-- <script type='text/javascript' src='menu/js/jquery.dcjqaccordion.2.7.min.js'></script> -->

    <!-- <link href="menu/css/skins/blue.css" rel="stylesheet" type="text/css" /> -->
    <!-- <link href="menu/css/skins/graphite.css" rel="stylesheet" type="text/css" /> -->
    <!-- <link href="menu/css/skins/grey.css" rel="stylesheet" type="text/css" /> -->
  
    <!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
        
  
    <!-- <script src="script.js"></script> -->
  
    <!-- <script src="jquery.sticky-kit.js "></script> -->
    <script type='text/javascript' src='https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/menu/js/jquery.cookie.js'></script>
    <script type='text/javascript' src='https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/menu/js/jquery.hoverIntent.minified.js'></script>
    <script type='text/javascript' src='https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/menu/js/jquery.dcjqaccordion.2.7.min.js'></script>

    <link href="https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/menu/css/skins/blue.css" rel="stylesheet" type="text/css" />
    <link href="https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/menu/css/skins/graphite.css" rel="stylesheet" type="text/css" />
    <link href="https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/menu/css/skins/grey.css" rel="stylesheet" type="text/css" />
    <link href="https://cdn.jsdelivr.net/gh/ryangrose/easy-pandoc-templates@948e28e5/css/elegant_bootstrap.css" rel="stylesheet" type="text/css" />
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
    <script src="https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/script.js"></script>
  
    <script src="https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/jquery.sticky-kit.js"></script>
    <meta name="generator" content="pandoc" />
  <meta name="author" content="Giovanni Enrico Loni" />
  <title>Intelligent Systems</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  
</head>
<body>

    
    <div class="navbar navbar-static-top">
    <div class="navbar-inner">
      <div class="container">
        <span class="doc-title">Intelligent Systems</span>
        <ul class="nav pull-right doc-info">
                    <li><p class="navbar-text">Giovanni Enrico
Loni</p></li>
                            </ul>
      </div>
    </div>
  </div>
    <div class="container">
    <div class="row">
            <div id="TOC" class="span3">
        <div class="well toc">

        <ul>
        <li><a href="#introduction-to-models-and-data"
        id="toc-introduction-to-models-and-data">Introduction to Models
        and Data</a>
        <ul>
        <li><a href="#introduction"
        id="toc-introduction">Introduction</a>
        <ul>
        <li><a href="#exploration"
        id="toc-exploration">Exploration</a></li>
        <li><a href="#modeling" id="toc-modeling">Modeling</a></li>
        <li><a href="#evaluation"
        id="toc-evaluation">Evaluation</a></li>
        <li><a href="#remarks" id="toc-remarks">Remarks</a></li>
        </ul></li>
        <li><a href="#data-and-transactions"
        id="toc-data-and-transactions">Data and transactions</a>
        <ul>
        <li><a href="#data-objects" id="toc-data-objects">Data
        objects</a></li>
        <li><a href="#attribute-types"
        id="toc-attribute-types">Attribute types</a></li>
        <li><a href="#distance-of-numeric-attributes"
        id="toc-distance-of-numeric-attributes">Distance of numeric
        attributes</a></li>
        </ul></li>
        <li><a href="#working-with-models"
        id="toc-working-with-models">Working with models</a>
        <ul>
        <li><a href="#training-and-generalization"
        id="toc-training-and-generalization">Training and
        generalization</a></li>
        <li><a href="#data-sampling" id="toc-data-sampling">Data
        sampling</a></li>
        <li><a href="#overfitting"
        id="toc-overfitting">Overfitting</a></li>
        <li><a href="#using-the-model" id="toc-using-the-model">Using
        the model</a></li>
        <li><a href="#training-and-testing"
        id="toc-training-and-testing">Training and testing</a></li>
        </ul></li>
        <li><a href="#useful-techniques"
        id="toc-useful-techniques">Useful techniques</a>
        <ul>
        <li><a href="#k-fold-cross-validation"
        id="toc-k-fold-cross-validation">K-fold
        cross-validation</a></li>
        <li><a href="#stratified-cross-validation-and-leave-one-out"
        id="toc-stratified-cross-validation-and-leave-one-out">Stratified
        cross-validation and leave-one-out</a></li>
        <li><a href="#bagging" id="toc-bagging">Bagging</a></li>
        <li><a href="#boosting" id="toc-boosting">Boosting</a></li>
        </ul></li>
        <li><a href="#essential-statistical-concepts"
        id="toc-essential-statistical-concepts">Essential Statistical
        Concepts</a>
        <ul>
        <li><a href="#dispersion-of-data"
        id="toc-dispersion-of-data">Dispersion of data</a></li>
        <li><a href="#box-plot" id="toc-box-plot">Box plot</a></li>
        <li><a href="#histogram-analysis"
        id="toc-histogram-analysis">Histogram analysis</a></li>
        <li><a href="#scatter-plot" id="toc-scatter-plot">Scatter
        plot</a></li>
        <li><a href="#x2-chi-squared-test"
        id="toc-x2-chi-squared-test"><span
        class="math inline">\(X^2\)</span> (Chi-squared) test</a></li>
        </ul></li>
        </ul></li>
        <li><a
        href="#data-exploration-and-preprocessing---data-mining-process"
        id="toc-data-exploration-and-preprocessing---data-mining-process">Data
        exploration and preprocessing - Data mining process</a>
        <ul>
        <li><a href="#data-cleaning" id="toc-data-cleaning">Data
        cleaning</a>
        <ul>
        <li><a href="#incomplete-data"
        id="toc-incomplete-data">Incomplete data</a></li>
        <li><a href="#noisy-data" id="toc-noisy-data">Noisy
        data</a></li>
        </ul></li>
        <li><a href="#data-discrepancy-detection"
        id="toc-data-discrepancy-detection">Data discrepancy
        detection</a></li>
        <li><a href="#data-redundancy" id="toc-data-redundancy">Data
        redundancy</a></li>
        <li><a href="#data-reduction" id="toc-data-reduction">Data
        reduction</a>
        <ul>
        <li><a href="#dimensionality-reduction"
        id="toc-dimensionality-reduction">Dimensionality
        reduction</a></li>
        <li><a href="#numerosity-reduction"
        id="toc-numerosity-reduction">Numerosity reduction</a></li>
        </ul></li>
        <li><a href="#advices-for-attribute-selection"
        id="toc-advices-for-attribute-selection">Advices for attribute
        selection</a></li>
        <li><a href="#heuristic-search-in-attribute-selection"
        id="toc-heuristic-search-in-attribute-selection">Heuristic
        search in Attribute Selection</a>
        <ul>
        <li><a href="#decision-tree-induction"
        id="toc-decision-tree-induction">Decision tree
        induction</a></li>
        <li><a href="#backward-elimination"
        id="toc-backward-elimination">Backward elimination</a></li>
        <li><a href="#forward-selection"
        id="toc-forward-selection">Forward selection</a></li>
        </ul></li>
        <li><a href="#data-transformation"
        id="toc-data-transformation">Data transformation</a>
        <ul>
        <li><a href="#normalization"
        id="toc-normalization">Normalization</a></li>
        </ul></li>
        </ul></li>
        <li><a href="#introduction-to-artificial-neural-networks"
        id="toc-introduction-to-artificial-neural-networks">Introduction
        to Artificial Neural Networks</a>
        <ul>
        <li><a href="#introduction-1"
        id="toc-introduction-1">Introduction</a>
        <ul>
        <li><a href="#what-is-a-neural-network"
        id="toc-what-is-a-neural-network">What is a Neural
        Network?</a></li>
        <li><a href="#neuron-model" id="toc-neuron-model">Neuron
        model</a></li>
        </ul></li>
        <li><a href="#artificial-neural-networks"
        id="toc-artificial-neural-networks">Artificial Neural
        Networks</a>
        <ul>
        <li><a href="#structure-of-an-ann"
        id="toc-structure-of-an-ann">Structure of an ANN</a></li>
        <li><a href="#artificial-neuron"
        id="toc-artificial-neuron">Artificial Neuron</a></li>
        <li><a href="#activation-functions"
        id="toc-activation-functions">Activation functions</a></li>
        <li><a href="#network-topology"
        id="toc-network-topology">Network topology</a></li>
        </ul></li>
        <li><a href="#network-training"
        id="toc-network-training">Network training</a>
        <ul>
        <li><a href="#delta-rule" id="toc-delta-rule">Delta
        rule</a></li>
        <li><a href="#momentum" id="toc-momentum">Momentum</a></li>
        <li><a href="#learning-algorithms"
        id="toc-learning-algorithms">Learning algorithms</a></li>
        </ul></li>
        <li><a href="#perceptron" id="toc-perceptron">Perceptron</a>
        <ul>
        <li><a href="#perceptron-learning-algorithm"
        id="toc-perceptron-learning-algorithm">Perceptron learning
        algorithm</a></li>
        <li><a href="#decision-boundary"
        id="toc-decision-boundary">Decision boundary</a></li>
        <li><a href="#xor-problem-with-perceptron"
        id="toc-xor-problem-with-perceptron">XOR problem with
        perceptron</a></li>
        <li><a href="#hidden-layers" id="toc-hidden-layers">Hidden
        layers</a></li>
        <li><a href="#improving-the-fitting"
        id="toc-improving-the-fitting">Improving the fitting</a></li>
        </ul></li>
        </ul></li>
        </ul>

        </div>
      </div>
            <div class="span9">

      
      <h1 id="introduction-to-models-and-data">Introduction to Models
and Data</h1>
<h2 id="introduction">Introduction</h2>
<p>When we talk about Artificial Intelligence, we’re talking about
<strong>building a model</strong>, starting from the available data,
that provides some <strong>knowledge</strong> about the world: this
knowledge must be <strong>readable</strong>,
<strong>interpretable</strong> and <strong>tangible</strong>, with a
practical use in various fields. Each IA algorithm on the following
operational pillars:</p>
<ul>
<li><strong>exploration</strong>;</li>
<li><strong>modeling</strong>,</li>
<li><strong>evaluation</strong>.</li>
</ul>
<h3 id="exploration">Exploration</h3>
<p>In this first step, data are crucial, serving as the main focus of
the algorithm to begin its analysis and the <strong>hypothesis
generation</strong>. The data, stored in databases, represent the
reality, and the effectiveness of the AI algorithm is based on the
ability to <strong>utilize data</strong>, to inform strategic decision
trough a process called <strong>business intelligence</strong>.
Obviously, to have an effective algorithm, we require data that are
logically consistent with themselves, and that are
<strong>representative</strong> of the reality: to achieve that, a good
strategy could be to gather data from different sources; this could be
challenging, but it’s a necessary step to have a good model. Lastly,
it’s crucial to understand how to <strong>leverage data</strong>, both
for validation and the testing phases of the model.</p>
<h3 id="modeling">Modeling</h3>
<p>In this phase, the figure of the <strong>knowledge engineer</strong>
appears: it has a good understanding of the data, the principles over
them and their defining parameters. The aim of this phase is to
<strong>estimate</strong> the parameters of the model, using statistics
and machine learning techniques. It’ also important to <strong>define
the model</strong>: based on the goal of the analysis, different
possibilities are available.</p>
<h4 id="descriptive-models">Descriptive models</h4>
<p>These models are typically identified by <strong>clustering</strong>
and <strong>association rules mining</strong> algorithms; they’re used
to <strong>describe the organizational structure and the data
distribution</strong>, and the main goal is a <strong>deeper
understanding</strong> of the data and its knowledge.</p>
<h4 id="predictive-models">Predictive models</h4>
<p>These models are usually implemented for
<strong>classification</strong> and <strong>regression</strong>
problems, and they’re used to <strong>predict</strong> the future
behavior of the data, based on the past one. This is made by exploiting
<strong>parameters</strong> and <strong>mechanisms</strong> within the
data, predicting how they will behave in the future.</p>
<h4 id="unsupervised-learning">Unsupervised learning</h4>
<p>This is a type of learning where the algorithm is given a set of data
and must find patterns and relationships within it, without any prior
knowledge, such as labels; it’s related to the descriptive models and
it’s used for <strong>building a model</strong>.</p>
<h4 id="supervised-learning">Supervised learning</h4>
<p>Typical of predictive models, where the data are labeled, and it’s a
method used to <strong>train the model</strong>.</p>
<h3 id="evaluation">Evaluation</h3>
<p>In this phase, we want want to <strong>evaluate the validity</strong>
of the model. When we’re dealing with predictive models, performances
are assessed by appropriate <strong>accuracy metrics</strong>; if
instead we’re dealing with descriptive ones, we have to evaluate the
<strong>quality</strong> of the model trough <strong>model-specific
metrics</strong>. Part of this phase is also the <strong>comparison
between models</strong>, made by knowledge engineers, trough statistical
tests and analysis of the results. At the end of this step, we can both
decide to <strong>deploy</strong> the model, or to refine it in order to
obtain better results.</p>
<h3 id="remarks">Remarks</h3>
<ul>
<li>Achieving a good model is a <strong>complex process</strong>, that
requires a meticulous approach to all the steps, such as leading to a
multiple execution of the same step for multiple times;</li>
<li>the knowledge of the application domain can aids during the modeling
phase, adding some <strong>domain-specific knowledge</strong> to the
model that could be not clear by simply analyzing the data;</li>
<li>the process of <strong>knowledge elicitation</strong>, where the
knowledge engineer has to <strong>extract the knowledge</strong> from
the domain expert, is crucial to have a good model;</li>
</ul>
<h2 id="data-and-transactions">Data and transactions</h2>
<p>Before going deep into the AI models, we have to understand what are
the data, and define a common way to refer to them. <em>Data</em> is
usually a general term that refers to the <strong>raw facts</strong>,
but in fact we call <strong>data set</strong> the collection of
<strong>data objects</strong>, which are the information that represent
an <strong>entity</strong>.</p>
<blockquote>
<p>Example: in a database of students, the data set is the collection of
all the students, and the data objects are the single students.</p>
</blockquote>
<h3 id="data-objects">Data objects</h3>
<p>Data objects are also called <strong>samples</strong>,
<strong>instances</strong> or <strong>records</strong>, and they’re
defined by <strong>atributes</strong>: we can image a samples as a
<strong>row of a table</strong>, and the attributes as the
<strong>columns</strong>. The attributes are also called
<strong>features</strong>, they’re the <strong>characteristics</strong>
of the data object and they can be expressed with different types.</p>
<h3 id="attribute-types">Attribute types</h3>
<p>The attributes can be of different types, such as:</p>
<ul>
<li><strong>nominal</strong>;</li>
<li><strong>binary</strong>;</li>
<li><strong>numeric</strong>, and we can distinguish between
<strong>interval</strong> and <strong>ratio</strong>.</li>
</ul>
<h4 id="nominal-attributes">Nominal attributes</h4>
<p>Intuitively, we define a nominal attribute as a
<strong>category</strong>, a <strong>name</strong> or a *name of a
thing**:</p>
<blockquote>
<p>Example: possible hair colors are blonde, brown, black, red, etc.</p>
</blockquote>
<p>Dealing with nominal attributes, we can define the
<strong>distance</strong> between two attributes <span
class="math inline">\(d(i, j)\)</span> as <span
class="math inline">\(d(i,j) = \frac{p - m}{p}\)</span> where <span
class="math inline">\(p\)</span> is the number of attributes and <span
class="math inline">\(m\)</span> is the number of matching
attributes.</p>
<blockquote>
<p>Example: given the variables <em>eye color</em> and <em>hair
color</em> and two elements $i = $ {green, blonde} and $j = $ {gree,
black}, we have <span class="math inline">\(d(i,j) = \frac{2-1}{2} =
0.5\)</span>.</p>
</blockquote>
<h4 id="binary-attributes">Binary attributes</h4>
<p>These are a particular case of nominal attributes, where we have only
two possible values, usually 0 and 1. If the possible outcomes are
equally important, they’re called <strong>symmetric</strong>; if instead
one of the two outcomes is more important, they’re called
<strong>asymmetric</strong>, and we’ll use the convention to assign the
value 1 to the most important outcome.</p>
<h4 id="ordinal-attributes">Ordinal attributes</h4>
<p>These are attributes that have a <strong>natural order</strong>, but
with a <strong>not defined distance</strong> between them. They’re used
to represent <strong>rankings</strong> or <strong>grades</strong>.</p>
<blockquote>
<p>Example: <em>size = {small, medium, large}</em>.</p>
</blockquote>
<p>Notice the <strong>importance</strong> of the order, and for this
reason we can treat them as <strong>interval-scaled</strong> attributes,
by mapping them into a set of <strong>integer values</strong>, and this
could be done with this algorithm:</p>
<ol type="1">
<li>assign a number to each value, starting from 1;</li>
<li>convert the values into a number between 0 and 1 using this formula:
<span class="math inline">\(v = \frac{x_i - 1}{x_M-1}\)</span>, where
<span class="math inline">\(x_i\)</span> is the value of the attribute
and <span class="math inline">\(x_M\)</span> is the maximum value.</li>
</ol>
<p>In this way, we can compute the <strong>distance</strong> using the
same methods of the interval-scaled attributes.</p>
<h4 id="interval-scaled-attributes">Interval-scaled attributes</h4>
<p>They are some sort of <strong>quantity</strong>, measured in a scale
of equal-sized units. Values have an order, but they lacks on the
definition of a <strong>zero point</strong>.</p>
<blockquote>
<p>Example: <em>temperature</em> measured in Celsius, where the 0
doesn’t mean the absence of temperature.</p>
</blockquote>
<h4 id="ratio-scaled-attributes">Ratio-scaled attributes</h4>
<p>They’re similar to the interval-scaled attributes, but they have a
<strong>true zero point</strong> (usually <strong>naturally
defined</strong>) and this allows us to compute the
<strong>ratio</strong> between two values.</p>
<blockquote>
<p>Example: <em>weight</em> measured in kilograms, where the 0 means the
absence of weight.</p>
</blockquote>
<h3 id="distance-of-numeric-attributes">Distance of numeric
attributes</h3>
<p>A popular measurement for the distance between two numeric attributes
is the <strong>Minkowski distance</strong>, defined as <span
class="math inline">\(d(i,j) = \sqrt[h]{\sum_{k=1}^{n} |x_{ik} -
x_{jk}|^h}\)</span>. When <span class="math inline">\(h = 1\)</span>, we
have the <strong>Manhattan distance</strong>, when <span
class="math inline">\(h = 2\)</span> we have the <strong>Euclidean
distance</strong>.</p>
<h2 id="working-with-models">Working with models</h2>
<p>When operating with models, we have to <strong>extract</strong> the
knowledge from the data; an example is the <strong>data mining</strong>
process, in which we have to <strong>extract patterns</strong> and
knowledge from massive data sets. In general, this process can be
summarized as follows:</p>
<figure>
<img src="../images/01/extractionProcess.png"
alt="The knowledge extraction process" />
<figcaption aria-hidden="true">The knowledge extraction
process</figcaption>
</figure>
<h3 id="training-and-generalization">Training and generalization</h3>
<p>We start knowing that <strong>training data contain the
knowledge</strong> that we want to extract: any AI algorithm is
dependent on these data, and the aim is to create a model that
<strong>mirror the data</strong> as close as possible, both for
predictive and descriptive models. Now appears clear that the
<strong>quality of the data</strong> is crucial in order to made the
extraction process effective. When we build a system, we must include a
<strong>training phase</strong>, where the data are used to
<strong>estimate the parameters</strong> of the model: the effectiveness
of this phase is evaluated by the <strong>capacity of the model to
generalize</strong> the knowledge, that is,classify correctly new data
that are not part of the training set.</p>
<h3 id="data-sampling">Data sampling</h3>
<p>Given a set of data, not all the records are used to create the
model, and this for two main reasons:</p>
<ul>
<li>for certain datasets, there are too much data available, and this
could lead to very long training times;</li>
<li>part of the data have to be used in the evaluation phase, to test
the model.</li>
</ul>
<p>A sampling strategy is used to select the data that will be used in
the training phase, considering the fact that our model should have a
good <strong>generalization capacity</strong>.</p>
<h3 id="overfitting">Overfitting</h3>
<p>In statistics, the <strong>overfitting</strong> problem arises when
the model has <strong>to many parameters</strong> relative to the amount
of data, leading to a model that is <strong>too complex</strong>: we
should avoid this situation, because <strong>simpler models have a
better generalization capacity</strong>. The overfitting problem is
usually solved by <strong>reducing the number of parameters</strong> of
the model, or by <strong>increasing the amount of data</strong>.</p>
<h3 id="using-the-model">Using the model</h3>
<p>Once the model is created, it’s ready to be used, following its
purpose.</p>
<h4 id="usage-of-predictive-models">Usage of predictive models</h4>
<p><strong>Predictive model</strong> can be used, where the functions
are like <strong>boxes</strong> with different <strong>opacity</strong>
(black, which is a neural network, grey, which is a rule-based system,
or white), that based on their structure and complexity can produce
outputs from the given input. The output of these models are obtained by
mechanisms, called <strong>inference engines</strong>, that use the
parameters obtained in the training.</p>
<h4 id="usage-of-descriptive-models">Usage of descriptive models</h4>
<p>These models use their parameters to <strong>describe</strong> the
characteristics of the data, such as <strong>centroid</strong> or
<strong>clusters</strong>, to represent similar data objects; we can use
these models to categorize new data objects, assigning them to the most
similar cluster. The accuracy of that classification, when data aren’t
labeled, is evaluated by the <strong>domain expert</strong>.</p>
<h3 id="training-and-testing">Training and testing</h3>
<p>Only a portion of the data (that have to be <strong>labeled</strong>
in order to create a predictive model), known as <strong>training
set</strong>, is used to train the model, while the remaining part, the
<strong>testing set</strong>, is used to evaluate the model. The testing
set is used to evaluate the <strong>generalization capacity</strong> of
the model, and it’s crucial to have a good model. Labeled data allow us
to evaluate the model, computing values such as <strong>average
error</strong> or <strong>accuracy</strong>.</p>
<p>Dealing with classification models, we instead use
<strong>misclassification rate</strong>, while for <strong>regression
models</strong> we use the <strong>error metrics</strong>.</p>
<figure>
<img src="../images/01/modelEvaulation.png" width="400"
alt="Model evaluation" />
<figcaption aria-hidden="true">Model evaluation</figcaption>
</figure>
<p>The training and test set have to be carefully, in order to ensure
randomness and to avoid <strong>bias</strong> towards a particular
class. In general:</p>
<ul>
<li>we must assure the <strong>independence</strong> of the training and
test set;</li>
<li>the ratio between sets is unbalanced, with the training set that is
usually larger than the test set.</li>
</ul>
<h2 id="useful-techniques">Useful techniques</h2>
<h3 id="k-fold-cross-validation">K-fold cross-validation</h3>
<p>This is one the most robust, and also used, approach used to
<strong>evaluate a model</strong>. Remember the fact that this is only
an evaluating procedure: it doesn’t gave any clue about how to improve
the model!. The technique is employed to <strong>reduce the model
dependence</strong> on data for both identification and evaluation.</p>
<p>Suppose to have a dataset, splittable in <span
class="math inline">\(N\)</span> instancies, the dataset is divided in
<span class="math inline">\(K\)</span> random partitions, also called
<strong>folds</strong>. The model is then trained and evaluated <span
class="math inline">\(K\)</span> times, using for each iteration <span
class="math inline">\(K-1\)</span> folds for training and the remaining
one for testing: this results in having <span
class="math inline">\(K\)</span> different models, each one evaluated
against a different test set, to assess the model’s generalization
capacity. The output of the procedure is <span
class="math inline">\(K\)</span> different values of
<strong>error</strong>, or <strong>accuracy</strong>, corresponding to
the <span class="math inline">\(K\)</span> different models: the average
of these values gives the <strong>cross-validation error</strong>.</p>
<p>Some general consideration about the standard deviation can be made:
if the <strong>standard deviation</strong> of the error is high, this
means that the model is <strong>sensitive</strong> to the data used for
training; on the other hand, if the <strong>standard deviation</strong>
is low, this means that the model appears to be
<strong>insensitive</strong> to the data used for training.</p>
<p>It’s important to remember that <strong>we cannot compare results
obtained with different evaluation techniques</strong>: the
<strong>cross-validation error</strong> is only a measure of the model’s
generalization capacity, and it’s not a measure of the model’s quality.
Lastly, remember that the <strong>cross-validation error</strong> isn’t
suitable in every situation, and it’s not always the best choice.</p>
<h4 id="example-of-k-fold-cross-validation">Example of K-fold
cross-validation</h4>
<p>The following figure shows an example of a 5-fold cross-validation,
where the dataset is divided into 4 folds:</p>
<figure>
<img src="../images/01/crossEvaluation.png" width="400"
alt="K-fold cross-validation" />
<figcaption aria-hidden="true">K-fold cross-validation</figcaption>
</figure>
<p>Every color represents a different fold, and we can clearly see the
creation of 4 different models, where each of them is created using 3
different folds for training and the remaining one for testing.</p>
<p>In output, we have 4 different statistics, that are used to evaluate
the model’s generalization capacity.</p>
<h3 id="stratified-cross-validation-and-leave-one-out">Stratified
cross-validation and leave-one-out</h3>
<p>Stratified CV is an extension of the standard cross-validation, and
it’s often applied when we’re dealing with classification problems.</p>
<p>In this technique, the subsets are partitioned in a way such that the
<strong>initial distribution of instancies</strong> of every class is
<strong>preserved</strong> in every fold. In case where the partitions
<span class="math inline">\(K\)</span> are equal to the number of
instancies <span class="math inline">\(N\)</span> (the number of
available instancies), the strategy is to <strong>leave one instance
out</strong>.</p>
<p>Then, <span class="math inline">\(N - 1\)</span> instancies are
iteratively used to train the model, and the generalization capacity is
evaluated for each of them. In the end, <strong>average error</strong>
is computed, considering the <span class="math inline">\(N\)</span>
results of the test set (which is the single instance left out).</p>
<h3 id="bagging">Bagging</h3>
<p>This technique is used to <strong>reduce the overtraining</strong>:
we train a set of <span class="math inline">\(T\)</span> models, such
that they are of different types but aiming to solve the same problem.
Each of them is trained on a <strong>unique training set</strong>, drawn
from the total data available, using the <strong>bootstrap
sampling</strong>. The latter let, for the same instance, to appears
multiple times in a single training set, while other instances could not
appear at all.</p>
<p>For each model <span class="math inline">\(n_i\)</span>, the number
of training samples <span class="math inline">\(N_i\)</span> is such
that <span class="math inline">\(N_i \leq N\)</span>, where <span
class="math inline">\(N\)</span> is the total number of instances
available. After the training phase of all the <span
class="math inline">\(T\)</span> models, we can combine the outcomes to
classify new instances:</p>
<ul>
<li>each model predicts the class of the new instance;</li>
<li>the final class will be the one that appears most frequently, using
the principle of <strong>majority voting</strong>.</li>
</ul>
<h3 id="boosting">Boosting</h3>
<p>Boosting is in fact an <strong>ensemble technique</strong>, which
have the aim to <strong>create a strong model</strong> from a set of
weak models. The idea is to build a model using <strong>the entire
training set</strong>, and then creating a <strong>second model</strong>
that attempts to <strong>correct the errors</strong> of the first one.
Models can be created and added sequentially, and new models focus on
the instances that were <strong>misclassified</strong> by the previous
ones.</p>
<p>This technique wants to give more importance to the instances that
are <strong>hard to classify</strong>; it also differs from the bagging
technique because the models aren’t trained in parallel (which results
in a independent training), but they’re trained
<strong>sequentially</strong>, where the new model is trained based on
the performance of the previous one.</p>
<p>The final model is made by taking the weighted vote, or average, of
the models created during the boosting process, where the weights are
assigned based on the individual performance of the models, having the
best models a higher weight.</p>
<figure>
<img src="../images/01/comparison.png" width="400"
alt="Visual comparison between bagging and boosting" />
<figcaption aria-hidden="true">Visual comparison between bagging and
boosting</figcaption>
</figure>
<h2 id="essential-statistical-concepts">Essential Statistical
Concepts</h2>
<p>In order to deal correctly with data, it is important to understand
some basic statistical and analysis concepts. In this section, we will
cover some fundamental concepts that will help you to understand the
data exploration process.</p>
<h3 id="dispersion-of-data">Dispersion of data</h3>
<p>Dispersion is a measure of how much the data points in a dataset
differ from the mean. We’ll use the <strong><span
class="math inline">\(k\text{-th}\)</span> percentile</strong> to
measure the dispersion of data that are numerically sorted. We say that
<em>value <span class="math inline">\(x_i\)</span> has the property that
<span class="math inline">\(k\)</span> percent of the data points are
less than or equal to <span class="math inline">\(x_i\)</span></em>.
From the previous we obtain that the <strong>median</strong> is the
<span class="math inline">\(50\text{-th}\)</span> percentile, and we
also define <strong>quartiles</strong> <span
class="math inline">\(Q_1\)</span> and <span
class="math inline">\(Q_3\)</span> as the <span
class="math inline">\(25\text{-th}\)</span> and <span
class="math inline">\(75\text{-th}\)</span> percentiles, respectively.
The <strong>interquartile range (IQR)</strong> is defined as <span
class="math inline">\(Q_3 - Q_1\)</span>.</p>
<h3 id="box-plot">Box plot</h3>
<p>A box plot is a graphical representation of the dispersion of data.
It is composed of a box that represents the interquartile range, and two
whiskers that represent the range of the data. The box plot also shows
the median as a line inside the box, as we can see in the following
figure.</p>
<figure>
<img src="../images/02/boxPlot.png" width="400" alt="Box plot" />
<figcaption aria-hidden="true">Box plot</figcaption>
</figure>
<h3 id="histogram-analysis">Histogram analysis</h3>
<p>A histogram is a graphical representation of the distribution of
data. It is composed of bars that represent the frequency of data points
in a given range, where the bars are called <strong>bins</strong>. The
main purpose of a histogram is to show the distribution of data, and it
is useful to identify patterns in the data. An example of a histogram is
shown in the following figure.</p>
<figure>
<img src="../images/02/histogram.png" width="400" alt="Histogram" />
<figcaption aria-hidden="true">Histogram</figcaption>
</figure>
<h3 id="scatter-plot">Scatter plot</h3>
<p>A scatter plot is a graphical representation of the relationship
between two variables. It is composed of points that represent the
values of the two variables, where the x-axis represents one variable
and the y-axis represents the other. The main purpose of a scatter plot
is to show the relationship between the two variables, and it is useful
to identify patterns in the data. An example of a scatter plot is shown
in the following figure.</p>
<figure>
<img src="../images/02/sactterPlot.png" width="400"
alt="Scatter plot" />
<figcaption aria-hidden="true">Scatter plot</figcaption>
</figure>
<h3 id="x2-chi-squared-test"><span class="math inline">\(X^2\)</span>
(Chi-squared) test</h3>
<p>The <span class="math inline">\(\chi^2\)</span> test is a statistical
test that is used to determine if two categorical variable, that are
independent by hypothesis, <span class="math inline">\(A\)</span> and
<span class="math inline">\(B\)</span> are somehow related in a given
population. The <strong>correlation coefficient</strong>, also called
<strong>Pearson’s correlation coefficient</strong>, is a measure of the
strength and direction of the linear relationship between two variables.
The correlation coefficient ranges from -1 to 1, where -1 indicates a
perfect negative linear relationship, 0 indicates no linear
relationship, and 1 indicates a perfect positive linear relationship,
and it’s calculated as <span class="math inline">\(r =
\frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i -
\bar{y})}{\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2 \sum_{i=1}^{n} (y_i -
\bar{y})^2}}\)</span>.</p>
<h1 id="data-exploration-and-preprocessing---data-mining-process">Data
exploration and preprocessing - Data mining process</h1>
<p>The data mining process is a systematic approach to extract knowledge
from data. It is composed of several steps that are executed in a
sequence, which are shown in the following figure.</p>
<figure>
<img src="../images/02/dataMining.png" width="400"
alt="Data mining process" />
<figcaption aria-hidden="true">Data mining process</figcaption>
</figure>
<h2 id="data-cleaning">Data cleaning</h2>
<p>Data that are extracted from the real world are
<strong>dirty</strong>. Different classes of <em>dirtiness</em> exist,
and they differ in the way they affect the data, and how they can be
detected and corrected.</p>
<h3 id="incomplete-data">Incomplete data</h3>
<p>Those are data that lack one or more attribute values: this might be
due to several reasons, such as the data collection process, equipment
failures, or human errors. In this case, missing data <strong>may need
to be inferred</strong>:</p>
<ul>
<li><strong>Ignore the tuple</strong>: this is usually done when class
label is missing, but it’s not an effective method when the percentage
of missing values varies from attribute to attribute;</li>
<li><strong>Fill in the missing value manually</strong>: this is usually
done when the percentage of missing values is small. Infeasible when the
number of missing values is large;</li>
<li><strong>automatic filling in of missing values</strong>: this can be
done using:
<ul>
<li>a <strong>global constant</strong>, such as “unknown” or even a new
class;</li>
<li>the <strong>attribute mean</strong> (or median, or mode);</li>
<li>the <strong>attribute mean</strong> for all samples belonging to the
<strong>same class</strong>;</li>
<li>the <strong>most probable value</strong> (e.g., using a decision
tree);</li>
</ul></li>
</ul>
<h3 id="noisy-data">Noisy data</h3>
<p>We define the noise as <em>random error or variance in a measured
variable</em>. This can be due to several reasons, such as data
collection errors, data transmission errors, or data entry errors. We
can deal with noisy data considering the source of the data that are
usually affected by noise, and the nature of the noise. Typical data
affected by noise are those from ambient intelligence, sensor networks,
and data streams: here the true signal amplitude (the y-value) change
<strong>rather smoothly</strong>, compared to the x-value. We can
operate a process called <strong>smoothing</strong> to remove the noise
from the data: data points of a signal are modified so that individual
points that are higher than the adjacent points are reduced, and those
that are lower are increased, naturally preserving the shape of the
signal, while reducing the noise and being naturally smoothed.</p>
<blockquote>
<p>Example: the simplest smoothing algorithm is the <strong>moving
average</strong>. We can use the <strong>rectangular sliding-average
smooth</strong>, that simply replace the point <span
class="math inline">\(i\)</span> with the mean of the <span
class="math inline">\(m\)</span> points around it, where <span
class="math inline">\(m\)</span> is called <strong>smooth
width</strong>. The formula for a 3-point rectangular sliding-average
smooth is <span class="math inline">\(y_i = \frac{1}{3} (x_{i-1} + x_i +
x_{i+1})\)</span>.</p>
</blockquote>
<h2 id="data-discrepancy-detection">Data discrepancy detection</h2>
<p>Data discrepancy detection is the process of identifying and
correcting discrepancies in the data. Different methods can be used to
detect discrepancies, such as:</p>
<ul>
<li>checking the <strong>metadata</strong> of the data;</li>
<li>check the <strong>field overload</strong>, typically results when
developers squeeze new attribute definitions into unused portions of
existing fields;</li>
<li>check the <strong>uniqueness rule</strong>: each value of a certain
attribute must be different from all the others;</li>
<li>check the <strong>consecutive rule</strong>: there cannot be missing
values between the lowest and the highest value of an attribute;</li>
<li>check the <strong>null rule</strong>: specifies the conditions under
which an attribute value can be null;</li>
<li>use commercial tools for both <strong>data scrubbing</strong> and
<strong>data auditing</strong>.</li>
</ul>
<h2 id="data-redundancy">Data redundancy</h2>
<p>Redundancy may appears when an integration of multiple databases is
performed:</p>
<ul>
<li>an attribute, or an object, may have different names in different
databases, also known as <strong>object identification
problem</strong>;</li>
<li>an attribute can be a <strong>derived attribute</strong>, that is,
it can be computed from other attributes;</li>
</ul>
<p>Redundant attributes can be also detected trough <strong>correlation
analysis</strong>; in general, extra care must be taken when dealing
during a data integration process, to reduce or entirely remove
redundancy.</p>
<h2 id="data-reduction">Data reduction</h2>
<p>Data reduction strategies are used to get a reduced representation of
the data set, while maintaining the same analytical power. This could be
done by several reason, such as no enough data storage, or to reduce the
time needed to perform the analysis. Two are the main strategies to
reduce data: <strong>dimensionality reduction</strong> and
<strong>numerosity reduction</strong>.</p>
<h3 id="dimensionality-reduction">Dimensionality reduction</h3>
<p>We start defining the dimensionality as the <em>number of attributes
in the data set</em>: adding dimensions to data, they become
increasingly <strong>sparse</strong>, and this lead to the <strong>curse
of dimensionality</strong>. This is a problem that arises when the data
set has a large number of dimensions, and it is difficult to analyze and
visualize the data: when dimensions increase, analysis such as density,
critical in clustering and outlier detection, become less powerful, and
this because the combinations of subspaces grow exponentially with the
number of dimensions.</p>
<figure>
<img
src="https://www.kdnuggets.com/wp-content/uploads/curse-dimensionality-2.png"
width="400" alt="Visualization od the curse of dimensionality" />
<figcaption aria-hidden="true">Visualization od the <em>curse of
dimensionality</em></figcaption>
</figure>
<p>Understood the problem, we can now define the <strong>dimensionality
reduction</strong> as the process of reducing the number of dimensions
in the data set. Note that the process also helps to <strong>reduce the
noise</strong> in the data, and <strong>remove redundant
information</strong>. The main techniques to reduce the dimensionality
are divided in two main classes: the supervised ones, such as
<strong>PCA</strong> or <strong>Wavelet Transform</strong>, and the
unsupervised ones, that are in matter of fact kind of optimization
problems, such as <strong>feature selection</strong> or <strong>feature
extraction</strong>.</p>
<h4 id="principal-component-analysis-pca">Principal Component Analysis
(PCA)</h4>
<p>The aim of this process is to project relevant attributes in a
different space, with a <em>hopefully</em> reduced space than the
original one. The projection should capture the largest amount of
variation in data, and this is done by finding the
<strong>eigenvectors</strong> of the <strong>covariance matrix</strong>
of the data. Using this technique we have to deal with the <strong>loss
of the feature meaning</strong>, that is explained in the further
example.</p>
<blockquote>
<p>Example: loss of feature meaning. Having a dataset containing data
about the weather, such as temperature, humidity, and wind speed, we can
apply PCA to reduce the dimensionality of the data. Reducing the dataset
dimensionality, we’re going to lose some features, that will be replaced
by a <strong>linear combination</strong> of the original features. This
means that the new features will be a mix of the original ones, and they
will not have a clear meaning.</p>
</blockquote>
<h4 id="attribute-and-feature-selection">Attribute and feature
selection</h4>
<p>This selection involves a search trough all possible combinations of
attributes, in order to find which subset of attributes is the most
relevant for the analysis, leading in fact to an optimization problem.
The main used algorithms are:</p>
<ul>
<li><strong>evaluator algorithms</strong>: they evaluate the goodness of
a subset of attributes using a <strong>quality function</strong>;</li>
<li><strong>search algorithms</strong>: they search for the best subset
of attributes using a <strong>search strategy</strong>, exploiting
heuristics to reduce the search space.</li>
</ul>
<h4 id="metrics-for-feature-importance">Metrics for feature
importance</h4>
<p>We know that features aren’t independent when we’re using them to
predict a result; however, we can use some metrics to understand which
features are more important than others. The most used metrics are the
<strong>correlation</strong>, the <strong>information gain</strong>, and
the <strong>Gini index</strong>, that show how much data entropy exists
between a given feature and the result.</p>
<p>Dealing with <strong>multiple feature evaluation</strong>, we use the
<strong>mRMR</strong> approach: <em>maximal Relevance, Minimal
Redundancy</em>. This approach is based on a filter feature measurement
criterion, which computes <strong>redundancy</strong> between features
in the subset, and <strong>correlation</strong> between features, and
class based on the <strong>mutual information</strong>, in order to
estimate the capability of making a good prediction for a given group of
features.</p>
<h4 id="filter-and-wrapper-methods">Filter and wrapper methods</h4>
<p>These are common optimization methods used to select the best subset
of features.</p>
<figure>
<img src="../images/02/wrapper.png" width="400"
alt="Filter and wrapper methods" />
<figcaption aria-hidden="true">Filter and wrapper methods</figcaption>
</figure>
<p>The <strong>search strategy</strong> is common on both methods, and
it’s based on heuristics: it identifies a subset which goes as input in
different modules, based on the method used. Using the <strong>filter
method</strong>, the subset is evaluated using a <strong>objective
function</strong>, that returns to the search module the goodness of the
subset, and other possibly useful statistics. The <strong>wrapper
method</strong> instead uses a <strong>predictive model</strong> to
evaluate the subset, and this model is trained on the subset, and then
tested on a validation set.</p>
<p>Take care of the model used in the wrapper method: we’re using a
classifier to obtain parameters for another classifier! The one used
during this evaluation phase should be very simple, using few (and
possibly different) parameters, compared to the one used in the final
model.</p>
<p>In both cases, we can stop the search when the <strong>quality
function</strong>, or the <strong>predictive model</strong>, reaches a
certain given stop condition.</p>
<h4 id="ranking-method">Ranking method</h4>
<p>This is the fastest method to select the best subset of features, but
also the one with the worst results. We can see the phases of the
ranking method in the following figure.</p>
<figure>
<img src="../images/02/ranking.png" width="400" alt="Ranking method" />
<figcaption aria-hidden="true">Ranking method</figcaption>
</figure>
<p>Examples of threshold can be, for instance, the <strong>number of
features</strong> to select, or a minimum <strong>quality
function</strong> value.</p>
<h3 id="numerosity-reduction">Numerosity reduction</h3>
<p>The aim of this process is to reduce the number of data instances,
while maintaining the same analytical power. The first operation we can
make is the <strong>sampling</strong>: we can select a subset of the
data, and use it for the analysis; this is typically done when dealing
with skewed data. Another methods are available: they’re called
<strong>mining algorithms</strong>, and they should be potentially
<strong>sub-linear</strong> in the size of the data set. The key idea to
have in mind is choose a representative subset of the data: operating a
random selection could lead to a non-representative subset, and this
could lead to a wrong analysis. To achieve a good results, different
sampling methods can be used:</p>
<ul>
<li><strong>random sampling</strong>: each data instance has the same
probability to be selected;</li>
<li><strong>sampling without replacement</strong>: each data instance
can be selected only once;</li>
<li><strong>sampling with replacement</strong>: each data instance can
be selected multiple times;</li>
<li><strong>stratified sampling</strong>: the data set is divided into
strata, and then a random sample is selected from each stratum.</li>
</ul>
<p>The latter is particularly useful when dealing with skewed data,
because it ensure that even the smallest stratum is represented in the
sample.</p>
<h2 id="advices-for-attribute-selection">Advices for attribute
selection</h2>
<p>The most important thing to understand is that <strong>we never want
to touch the training set</strong>. Suppose to have a dataset, and a
certain reduction is being applied to it. We consider the training set
as a <em>part of the future</em>, because our model should never see the
test set during the training phase. If we included the test set during
the analysis for the attribute selection, we’re somehow
<strong>including the test set inside the model</strong>, and this is a
<strong>bad practice</strong>. We’re, in fact, introducing a bias in the
model, using <em>future data</em> that are impossible to obtain in the
real world. Given that rationale, we’ll refer at the following figure,
that shows the correct way to perform the attribute selection, as the
<strong>golden rule</strong>.</p>
<figure>
<img src="../images/02/attributeSelection.png" width="400"
alt="Golden rule for attribute selection" />
<figcaption aria-hidden="true">Golden rule for attribute
selection</figcaption>
</figure>
<h2 id="heuristic-search-in-attribute-selection">Heuristic search in
Attribute Selection</h2>
<h3 id="decision-tree-induction">Decision tree induction</h3>
<p>The decision tree induction is an automatic way to perform a search;
to explain the concept, we can consider the following example.</p>
<figure>
<img src="../images/02/decisionTree.png" width="400"
alt="Decision tree induction" />
<figcaption aria-hidden="true">Decision tree induction</figcaption>
</figure>
<p>Take an initial set of attributes <span class="math inline">\(\{A_1,
A_2, A_3, A_4, A_5, A_6\}\)</span>, and compute metrics on these
attributes (e.g., the <strong>information gain</strong>, usually chosen
based on the problem we’re dealing with), and we choose an attribute
based on that metric. Then we start to build our tree from the root: it
will have a condition based on the chosen attribute, and the branches
will develop themselves based on other attributes. The aim of the
technique is to find the <strong>splitting point</strong> as the point
such that we split the attribute set in two parts, and we chose the one
that represents better the data. The process is repeated until we reach
a stopping condition, that could be the <strong>maximum depth</strong>
of the tree, or the <strong>minimum number of samples</strong> in a
leaf. Following the example, the reduced attribute set will be <span
class="math inline">\(\{A_1, A_4, A_5\}\)</span>.</p>
<h3 id="backward-elimination">Backward elimination</h3>
<p>This is a <strong>greedy</strong> algorithm, that starts with the
full set of attributes, and then removes one attribute at a time, based
on a certain criterion such as the <strong>information gain</strong>.
The process is repeated until a stopping condition is reached, such as
the <strong>minimum number of attributes</strong>.</p>
<h3 id="forward-selection">Forward selection</h3>
<p>This is another <strong>greedy</strong> algorithm, that starts with
an empty set of attributes, and then adds one attribute at a time, based
on a certain criterion. The pseudo-code for the algorithm is the
following:</p>
<ol type="1">
<li>initialize: set <span class="math inline">\(F = \{f_i | i = 1,
\ldots, N\}\)</span> the initial set of the features, and <span
class="math inline">\(S = \emptyset\)</span>;</li>
<li>calculate, for each attribute, the importance metric <span
class="math inline">\(M(f_i, Y)\)</span> with respect to the input <span
class="math inline">\(Y\)</span>;</li>
<li>select the first feature <span class="math inline">\(f_i\)</span>
that maximizes <span class="math inline">\(M(f_i, Y)\)</span>, and set
<span class="math inline">\(F = F \setminus \{f_i\}\)</span>, and <span
class="math inline">\(S = S \cup \{f_i\}\)</span>;</li>
<li><strong>greedy selection</strong>: repeat the process until a
stopping condition is reached, such as the <strong>minimum number of
attributes</strong>.
<ol type="1">
<li>calculate the importance metric <span class="math inline">\(M(f_i
\cup S, Y)\)</span> for each attribute <span class="math inline">\(f_i
\in F\)</span>;</li>
<li>select the next feature <span class="math inline">\(f_i\)</span>
that maximizes <span class="math inline">\(M(f_i \cup S, Y)\)</span>,
and set <span class="math inline">\(F = F \setminus \{f_i\}\)</span>,
and <span class="math inline">\(S = S \cup \{f_i\}\)</span>.</li>
</ol></li>
<li>output the set <span class="math inline">\(S\)</span>.</li>
</ol>
<h2 id="data-transformation">Data transformation</h2>
<p>Data transformation is the process of <strong>changing the format,
the structure or the values of the data</strong>. These techniques are
usually applied to ensure <strong>more efficient data analysis</strong>
and better <strong>knowledge extraction</strong>.</p>
<h3 id="normalization">Normalization</h3>
<p>The aim of this process is to scale the data in a certain range,
usually <span class="math inline">\([0, 1]\)</span>, or <span
class="math inline">\([-1, 1]\)</span>. This is done to avoid that some
attributes have a <strong>higher weight</strong> in the analysis,
compared to others. The most used normalization techniques are:</p>
<ul>
<li><strong>min-max normalization</strong>: the data are scaled in the
range <span class="math inline">\([\text{new-min}_A,
\text{new-max}_A]\)</span>, using the formula <span
class="math inline">\(x&#39; = \frac{v - \text{min}_A}{\text{max}_A -
\text{min}_A}\cdot (\text{new-max}_A - \text{new-min}_A) +
\text{new-min}_A\)</span>;</li>
<li><strong>z-score normalization</strong>: the data are scaled using
the formula <span class="math inline">\(x&#39; = \frac{v -
\mu_A}{\sigma_A}\)</span>, where <span
class="math inline">\(\mu_A\)</span> is the mean of the attribute <span
class="math inline">\(A\)</span>, and <span
class="math inline">\(\sigma_A\)</span> is the standard deviation of the
attribute <span class="math inline">\(A\)</span>.</li>
</ul>
<h1 id="introduction-to-artificial-neural-networks">Introduction to
Artificial Neural Networks</h1>
<h2 id="introduction-1">Introduction</h2>
<h3 id="what-is-a-neural-network">What is a Neural Network?</h3>
<p>An <strong>Artificial Neural Network (ANN)</strong> is an abstract
simulation of a human nervous system, that contains a collection of
neurons connected with each other trough connections called
<strong>axons</strong>. The aim of a neural network is to simulate
neurons and connections, resembling the human brain.</p>
<h3 id="neuron-model">Neuron model</h3>
<p>Many neurons have a structure called <strong>dendrites</strong> that
receive signals from other neurons trough <strong>synapses</strong>. The
neuron processes the signals and sends the output signal through the
<strong>axon</strong>, which is basically an electrical impulse: it can
be <strong>excitatory</strong> or <strong>inhibitory</strong>, and in
case of excitatory signals, the neuron will generate informational
messages to other neurons.</p>
<p>It’s estimated that around 100 billion neurons are in the human
brain, and each neuron can be connected to thousands of other neurons:
their switching time is significantly slower than the switching time of
a computer, but the connectivity is much higher, up to hundreds times
more.</p>
<h2 id="artificial-neural-networks">Artificial Neural Networks</h2>
<h3 id="structure-of-an-ann">Structure of an ANN</h3>
<p>An NNN is composed by a set of <strong>neurons</strong> and
<strong>weighted connections</strong> between them, plus a series of
thresholds or <strong>activation levels</strong>. During the design of
an ANN, we have to take into account the <strong>number and type of
neurons</strong>, the <strong>morphology of the network</strong>, the
<strong>weights</strong> and the <strong>training examples</strong>, in
terms of network inputs and outputs.</p>
<h3 id="artificial-neuron">Artificial Neuron</h3>
<p>The scheme of an artificial neuron is shown in the following
image:</p>
<p><img
src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/60/ArtificialNeuronModel_english.png/400px-ArtificialNeuronModel_english.png"
width="400" alt="Artificial Neuron" />{width=400px}</p>
<p>Note that the artificial neuron has a set of <strong>inputs</strong>
<span class="math inline">\(x_1, x_2, \ldots, x_n\)</span> and a set of
<strong>weights</strong> <span class="math inline">\(w_1, w_2, \ldots,
w_n\)</span>, which are real number such that, if positive, they are
excitatory, and if negative, they are inhibitory; these weights are
related to the corresponding inputs. The neuron computes the
<strong>weighted sum</strong> of the inputs and weights as a linear
combination <span class="math inline">\(a = \sum_{i=1}^{n} w_i \cdot
x_i\)</span>, that goes into an <strong>activation function</strong>
<span class="math inline">\(\phi(a)\)</span>. The output of the neuron
can be a real number, both non-limited or limited to a certain range, or
even a discrete value. Lastly, there also is a binary threshold function
such that, if the output is above a certain threshold <span
class="math inline">\(\theta_i\)</span>, the neuron will fire. In fact,
the latter is a generalization of a <strong>step function</strong> for a
given threshold. Typically, the threshold is also subtracted from the
weighted sum, in order to have the function <span
class="math inline">\(y = f(\sum_{i=1}^{n} w_i \cdot x_i - \theta_i) =
f(\sum_{i=0}^{n}w_i \cdot x_i)\)</span> <strong>centered in
zero</strong>. If a threshold assumes a negative value, is called
<strong>bias</strong>, and it’s considered as a weight connected to a
unit that always outputs 1 (note that in the previous equation we added
the index 0 to the sum, which is the bias).</p>
<h3 id="activation-functions">Activation functions</h3>
<p>In the following image, we can see some examples of activation
functions:</p>
<figure>
<img src="../images/03/activationFunctions.png" width="400"
alt="Activation functions" />
<figcaption aria-hidden="true">Activation functions</figcaption>
</figure>
<h3 id="network-topology">Network topology</h3>
<p>The topology depends on <strong>how neurons are connected which each
other</strong>: if they’re arranged in a <strong>hierarchical</strong>
way, such that the neurons are connected only with <strong>adjacent
layers</strong>, we have a <strong>feedforward network</strong>. If the
neurons are connected with <strong>non-adjacent layers</strong>, we have
a <strong>recurrent network</strong>. We can see an example of these two
types of networks in the following image:</p>
<figure>
<img src="../images/03/hierarchy.png" width="400"
alt="Feedforward and Recurrent Networks" />
<figcaption aria-hidden="true">Feedforward and Recurrent
Networks</figcaption>
</figure>
<p>Note how the firsts neurons are connected only with the second layer
in the feedforward network, while in the recurrent network they this
rule is not respected; in particular, in this case, the network is
implementing a sort of <em>memory</em>, with a structure that resembles
a <strong>flip-flop</strong>.</p>
<h2 id="network-training">Network training</h2>
<p>When we want to use a neural network to deal with classification and
regression problems, we resort to a <strong>supervised learning</strong>
process, which modifies the weights of the network by applying a set of
<strong>labeled training examples</strong>: each sample consists of an
input and the corresponding target output. During the training phase,
the network is fed with the input, and the output is compared with the
target output, then the weights are adjusted in order to
<strong>minimize the error</strong>. The training process is repeated
until there are no significant changes in the weights.</p>
<h3 id="delta-rule">Delta rule</h3>
<p>The <strong>delta rule</strong> is a simple algorithm that modifies
the weights of the network in order to minimize the error. The rule is
based on the <strong>gradient descent</strong> method, which is a method
to find a local minimum of a function. The delta rule says that the
adjustment to be made is <span class="math inline">\(\Delta w_i = \eta
\cdot \delta \cdot x_i\)</span>, where <span
class="math inline">\(\eta\)</span> is the <strong>learning
rate</strong>, and <span class="math inline">\(\delta\)</span> is
defined as <span class="math inline">\(\delta = t - y\)</span>, <span
class="math inline">\(t\)</span> is the target output, <span
class="math inline">\(y\)</span> is the actual output, and <span
class="math inline">\(x_i\)</span> is the input. Note that this is a
<strong>recursive rule</strong>, and the formula can be rewritten as
<span class="math inline">\(w_i (n+1) = w_i (n) + \Delta
w_i(n)\)</span>.</p>
<p>We define the <strong>error for the k-th sample</strong> as <span
class="math inline">\(E_k = \frac{1}{2} \sum_{i=1}^{n} (t_i -
y_i)^2\)</span>, where <span class="math inline">\(t_i\)</span> is the
target output and <span class="math inline">\(y_i\)</span> is the actual
output. The <strong>total error</strong> is defined as <span
class="math inline">\(E = \sum_{k=1}^{m} E_k\)</span>, where <span
class="math inline">\(m\)</span> is the number of samples.</p>
<h4 id="main-idea">Main idea</h4>
<p>TO understand the main idea behind the delta rule, we have to give a
mathematical interpretation of the error function, that is the fact that
it lies in the same space of the weights. This is crucial because, after
the start where the weights are randomly assigned, the algorithm adjusts
them in a direction <strong>towards a lower overall error</strong>: in
other words, the weights are modified in the direction of the
<strong>steep descent</strong> of the error surface, and we’ll prove the
convergence of the algorithm in the next section.</p>
<h4 id="proof-of-convergence">Proof of convergence</h4>
<p>For the proof, we start rewriting the delta rule as <span
class="math inline">\(\Delta w_{ij} = -G = - \frac{\partial E}{\partial
w_{ij}}\)</span>, and we can visually interpret this in the following
image:</p>
<figure>
<img src="../images/03/function.png" width="400"
alt="Visualization of the delta rule" />
<figcaption aria-hidden="true">Visualization of the delta
rule</figcaption>
</figure>
<p>Iterating the rule, we move <em>downhill</em> in <span
class="math inline">\(E\)</span>, until we reach a minimum <span
class="math inline">\(G=0\)</span>.</p>
<p>Now we write the actual output of the network as <span
class="math inline">\(y_i = \sum_ix_iw_{ij}\)</span>, so as a linear
combinations of the inputs and weights. We can now compute the partial
derivative, applying the <em>chain rule</em>, and we obtain <span
class="math inline">\(\frac{\partial E}{\partial w_{ij}} =
\frac{\partial E}{\partial y_i}\cdot \frac{\partial y_i}{\partial
w_{ij}}\)</span>. Then, we write <span class="math inline">\(- \delta =
\frac{\partial E}{\partial y_i}\)</span>, and we can compute the partial
derivative of the output with respect to the weights as <span
class="math inline">\(\frac{\partial y_i}{\partial w_{ij}} =
x_i\)</span>. So, we can rewrite the delta rule, adding the
<strong>learning rate</strong> <span
class="math inline">\(\eta\)</span>, as <span
class="math inline">\(\Delta w_{ij} = \eta \cdot \delta \cdot
x_i\)</span>, and we can prove that the algorithm converges to a
minimum.</p>
<p>The definition of <span class="math inline">\(\delta\)</span> came
out by calculating the partial derivative of the error with respect to
the output, considering a single instance: <span
class="math inline">\(\frac{\partial E}{\partial y_i} =
\frac{\partial}{\partial y_i} \frac{1}{2} (t_i - y_i)^2 = - (t_i - y_i)
= - \delta\)</span>.</p>
<h4 id="the-learning-rate">The learning rate</h4>
<p>The learning rate <span class="math inline">\(\eta\)</span> is a
crucial parameter in the delta rule, because it determines the
<strong>step size</strong> of the algorithm. If the learning rate is too
high, the algorithm may oscillate around the minimum, and if it’s too
low, the algorithm may take too long to converge. In real cases, its
value is determined experimentally, and it can vary overtime, getting
smaller as the training progresses.</p>
<h3 id="momentum">Momentum</h3>
<p>In real-world problems, it can happens that the error surface is not
smooth, and the algorithm may get stuck in a local minimum. To overcome
this issue, we can introduce a <strong>momentum</strong> <span
class="math inline">\(m\)</span> term in the delta rule, such that the
new formula is <span class="math inline">\(\Delta w_{ij}(n+1) = \eta
\cdot \delta \cdot x_i + m \cdot \Delta w_{ij}(n)\)</span>. The momentum
term is a fraction of the previous weight update, and it’s domain is
<span class="math inline">\(]0,1[\)</span>, and the actual value is
determined by trial and error.</p>
<h3 id="learning-algorithms">Learning algorithms</h3>
<p>We divide the learning algorithms in three categories:</p>
<ul>
<li><strong>online learning</strong>, where the weights are updated
after each sample;</li>
<li><strong>batch learning</strong> (or <em>offline</em>), where the
gradient is computed for all the samples, and the weights are updated at
the end of the process;</li>
<li><strong>mini-batch learning</strong>, where the gradient is computed
for a subset of the samples, and the weights are updated at the end of
the process.</li>
</ul>
<p>The last two methods provide both a more precise estimation of the
gradient vector, and possibly a faster computation, given the fact that
these processes can be parallelized; on the other hand, the online
learning is more flexible, because it doesn’t need a fixed training set,
requires less memory and, lastly, it has the ability to escape from
local minima under certain conditions.</p>
<h2 id="perceptron">Perceptron</h2>
<p>A perceptron is a <strong>single neuro</strong>n with an hard limiter
as activation function, such as sign or step function , and it’s used to
solve <strong>binary classification</strong> problems. It can be trained
to correctly classify training examples from classes <span
class="math inline">\(C_1\)</span> abd <span
class="math inline">\(C_2\)</span> by assigning inputs to <span
class="math inline">\(C_1\)</span> if the output is positive, and to
<span class="math inline">\(C_2\)</span> if the output is negative.</p>
<p>The classification is made by using a <strong>straight line</strong>
in the input space, that represents the <strong>decision
boundary</strong> between the two classes, which are the regions that
are separated by the line.</p>
<figure>
<img src="../images/03/perceptron1.png" width="400" alt="Perceptron" />
<figcaption aria-hidden="true">Perceptron</figcaption>
</figure>
<h3 id="perceptron-learning-algorithm">Perceptron learning
algorithm</h3>
<p>A pseudo-code of the perceptron learning algorithm is the
following:</p>
<ol type="1">
<li>Initialize the weights to 0 or small random values;</li>
<li>choose an appropriate learning rate <span
class="math inline">\(\eta\)</span>;</li>
<li>until the stopping criterion is met (such as the number of
iterations or the fact that weights don’t change significantly):
<ol type="1">
<li>for each training sample <span class="math inline">\((x_i,
t_i)\)</span>:
<ol type="1">
<li>compute the output <span class="math inline">\(y_i =
f(w,x)\)</span>;</li>
<li>if <span class="math inline">\(y_i = t_i\)</span>, continue;</li>
<li>if <span class="math inline">\(y_i \neq t_i\)</span>, update the
weights as <span class="math inline">\(w = w + \eta \cdot (t_i - y_i)
\cdot x_i\)</span>, where <span class="math inline">\(t_i\)</span> is
the target output.</li>
</ol></li>
</ol></li>
</ol>
<h3 id="decision-boundary">Decision boundary</h3>
<p>We can make a mathematical consideration about the points that lie in
the decision boundary: they all have the same inner product with the
weight vector. From this, we can say that they have the <strong>same
projection</strong> on the weight vector, so they must lie on a line
that is <strong>orthogonal</strong> to the weight vector. Look at these
two images:</p>
<figure>
<img src="../images/03/boundaries.png" width="400"
alt="Decision boundary" />
<figcaption aria-hidden="true">Decision boundary</figcaption>
</figure>
<p>The red dashed line is the decision boundary, so note that both
points <span class="math inline">\(p_1\)</span> and <span
class="math inline">\(p_2\)</span> are incorrectly classified. What
happens if we choose one of the as training sample to update the
weights?</p>
<ul>
<li>choosing <span class="math inline">\(p_1\)</span>, it has target
<span class="math inline">\(t=1\)</span>, so weights are slightly
updated in the direction of <span
class="math inline">\(p_1\)</span>;</li>
<li>choosing <span class="math inline">\(p_2\)</span>, it has target
<span class="math inline">\(t=-1\)</span>, so weights are slightly
updated in the direction of <span
class="math inline">\(p_2\)</span>.</li>
</ul>
<p>Note that in both cases, the new boundary is better than the previous
one, and this because <strong>the perceptron learning algorithm is a
linear algorithm is guaranteed to converge in a finite number of steps,
if the problem us linearly separable</strong>.</p>
<h3 id="xor-problem-with-perceptron">XOR problem with perceptron</h3>
<p>The XOR problem is a problem that can’t be solved by a single
perceptron, because the classes are not linearly separable. In fact, the
XOR problem is a <strong>non-linear</strong> problem, and it can be
solved by adding a <strong>hidden layer</strong> to the network, or by
using a specific activation function.</p>
<p>If we want to resolve the problem by using two separating lines,
we’ll need <strong>two perceptrons</strong> to represents the two lines,
and a third neuron to combine the outputs of the first two neurons.
We’ll obviously need a <strong>threshold</strong> for each neuron, and
the output of the third neuron will be the final output of the
network.</p>
<figure>
<img src="../images/03/XOR1.png" width="400"
alt="Network with hidden layer" />
<figcaption aria-hidden="true">Network with hidden layer</figcaption>
</figure>
<p>The effect of the first layer on the weights is such that now the
problem that goes into the second layer is linearly separable, and the
second layer will be able to solve the problem.</p>
<h3 id="hidden-layers">Hidden layers</h3>
<p>In general, we observe that adding a hidden layer to the network, the
network itself became able to model regions with a number of sides at
most equal to the number of neurons in the hidden layer. In fact, the
hidden layer is able to model <strong>non-linear</strong> regions, and
the output layer is able to combine the results of the hidden layer to
solve the problem. If we instead add a second hidden layer, the network
will be able to model regions that are arbitrarily complex.</p>
<h3 id="improving-the-fitting">Improving the fitting</h3>
<p>Observe the following image:</p>
<figure>
<img src="../images/03/linearFit.png" width="400"
alt="An example of linear fitting" />
<figcaption aria-hidden="true">An example of linear fitting</figcaption>
</figure>
<p>We can clearly see that data are not evenly distribuited around the
straight line, so we can achieve a better approximation by using a
<strong>hidden layer</strong>, consisting in a single neuron with
<strong>tanh</strong> as activation function. The result is shown in the
following image:</p>
<figure>
<img src="../images/03/tanh.png" width="400"
alt="An example of non-linear fitting" />
<figcaption aria-hidden="true">An example of non-linear
fitting</figcaption>
</figure>
<p>This approach can be used to approximate even more complex functions,
simply adding more neurons to the hidden layer. However, we need to
consider the negative aspect of this approach: too <strong>many
neurons</strong> in a single hidden layer, or simply too <strong>many
hidden layers</strong>, can have a <strong>negative impact on the
network performance</strong>. The best practice is to start with a
network with a reasonable minimum number of hidden neurons, and then
increase the number of neurons only if the network doesn’t perform well.
Remember that a network with a hidden layer that contains enough neurons
can approximate any continuous function.</p>
            </div>
    </div>
  </div>
  <script src="https://vjs.zencdn.net/5.4.4/video.js"></script>

</body>
</html>
